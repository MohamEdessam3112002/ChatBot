from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_classic.output_parsers import PydanticOutputParser
from langchain_classic.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableParallel
from pydantic import BaseModel, Field
import gradio as gr
from dotenv import load_dotenv
from os import getenv, path

load_dotenv()
api_key = getenv("GOOGLE_API_KEY")

embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")

if path.exists("mall_db"):
    db = Chroma(persist_directory="mall_db", embedding_function=embeddings)
else:
    loader = CSVLoader("products.csv")
    documents = loader.load()
    db = Chroma.from_documents(documents, embeddings, persist_directory="mall_db")

retriever = db.as_retriever(search_kwargs={"k": 3})

chat = chat = ChatGoogleGenerativeAI(
    model = "gemini-2.0-flash",
    google_api_key=api_key,
    temperature=0.7
)

response_template = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ù…ÙˆÙ„ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ÙÙ‡Ù… Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù‚Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù†ÙØ³Ùƒ

 Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
{response_chat_history}

 Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª:
{context}

 Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{question}

"""

response_prompt = PromptTemplate(
    template=response_template,
    input_variables=["response_chat_history", "context", "question"],
)

response_memory = ConversationBufferMemory(memory_key="response_chat_history", return_messages=False)

class ProductInfo(BaseModel):
    product: str = Field(description = "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬")
    price: float = Field(description="Ø³Ø¹Ø± Ø§Ù„Ù…Ù†ØªØ¬")
    store: str = Field(description="Ø§Ø³Ù… Ø§Ù„Ù…ØªØ¬Ø±")

class ProductList(BaseModel):
    items: list[ProductInfo]

parser = PydanticOutputParser(pydantic_object=ProductList)
format_instructions = parser.get_format_instructions()

json_template = """
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ù…ÙˆÙ„ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ.
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ÙÙ‡Ù… Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
Ø¬Ø§ÙˆØ¨ Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·.

 Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
{json_chat_history}

 Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª:
{context}

 Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
{question}

{format_instructions}
"""

json_prompt = PromptTemplate(
    template=json_template,
    input_variables=["json_chat_history", "context", "question"],
    partial_variables={"format_instructions": format_instructions},
)

json_memory = ConversationBufferMemory(memory_key="json_chat_history", return_messages=False)

response_chain = response_prompt | chat
json_chain = json_prompt | chat | parser
combined_chain = RunnableParallel({
    "response": response_chain,
    "json": json_chain
})

def mall_bot(message, history):
    question = message.strip()
    if not question: return ""

    response_chat_history = response_memory.load_memory_variables({}).get("response_chat_history", "")
    json_chat_history = json_memory.load_memory_variables({}).get("json_chat_history", "")

    docs = retriever.invoke(question)
    context = "\n".join([d.page_content for d in docs])

    answer = combined_chain.invoke({
        "response_chat_history": response_chat_history,
        "json_chat_history": json_chat_history,
        "context": context,
        "question": question
    })

    # Ø§Ù„Ø±Ù…Ø² Ø¯Ù‡ (u\200f) Ù‡Ùˆ Ø­Ø±Ù ØºÙŠØ± Ù…Ø±Ø¦ÙŠ Ø¨ÙŠØ¬Ø¨Ø± Ø§Ù„Ø³Ø·Ø± ÙŠØ¨Ù‚Ù‰ ÙŠÙ…ÙŠÙ†
    rtl_mark = "\u200f" 
    
    json_items_str = "\n".join(
        # Ø¶ÙÙ†Ø§ Ø§Ù„Ø±Ù…Ø² Ù‚Ø¨Ù„ ÙƒÙ„ Ø³Ø·Ø±
        [f"{rtl_mark}- {item.product} : {item.price} LE ({item.store})" for item in answer["json"].items]
    )
    
    json_memory.save_context(
        {"input": question},
        {"output": json_items_str}
    )

    response_memory.save_context(
        {"input": question},
        {"output": answer["response"].content}
    )

    return answer["response"].content

rtl_css = """
.message {
    text-align: right !important;
    direction: rtl !important;
}
/* ØªØ¸Ø¨ÙŠØ· Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… (Ø§Ù„Ù†Ù‚Ø·) Ø¹Ø´Ø§Ù† ØªÙŠØ¬ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ† */
ul {
    direction: rtl !important;
    text-align: right !important;
}
li {
    direction: rtl !important;
    text-align: right !important;
}
"""

demo = gr.ChatInterface(
    fn=mall_bot,
    title="Mall Assistant Bot ğŸ›’",
    description="Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù„Ù…ÙˆÙ„ØŒ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±.",
    examples=["Ø³Ø¹Ø± Ø§Ù„Ø§ÙŠÙÙˆÙ† ÙƒØ§Ù…ØŸ", "Ø¹Ù†Ø¯ÙƒÙ… Ù„Ø§Ø¨ØªÙˆØ¨ Ø¯ÙŠÙ„ØŸ", "Ø§ÙŠÙ‡ Ø£Ø±Ø®Øµ Ø´Ø§Ø´Ø©ØŸ"],
    theme="soft",
    type="messages",
    css=rtl_css 
)

if __name__ == "__main__":
    demo.launch()